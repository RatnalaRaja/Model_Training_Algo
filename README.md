
#  Simple Linear Regression 

This README is designed to help you revise and understand **Simple Linear Regression** using a real dataset: height and weight. It includes code breakdown, visual interpretation, and explanation of key metrics.

---

##  Regression Equation

The model tries to find the best straight line:
```
Y = mX + b
```
- `Y`: Predicted value (e.g., Height)
- `X`: Input feature (e.g., Weight)
- `m`: Slope (coefficient)
- `b`: Intercept

---


```
                ^
                |                       Actual Y
                |                        ‚óè
  Height (Y)    |                   ‚óè   ‚óè
                |              ‚óè   |   ‚óè
                |         ‚óè   |   ‚óè
                |    ‚óè    |  ‚óè
                |_________|_____________________>
                        Weight (X)
                        |-----> Predicted Y
```

The model tries to **minimize vertical distances (errors)** between the actual points and the regression line.

---

##  What is R¬≤ (R-squared)?

**R¬≤ (Coefficient of Determination)** shows how well the line fits the data.

###  Formula:
```
R¬≤ = 1 - (SSR / SST)
```
- **SSR**: Sum of Squares of Residuals
- **SST**: Total Sum of Squares

###  Meaning:
- R¬≤ = 1 ‚Üí perfect fit
- R¬≤ = 0 ‚Üí model explains nothing
- R¬≤ = 0.8 ‚Üí 80% of variation in Y is explained by X

---

##  What is Adjusted R¬≤?

Adjusted R¬≤ considers the number of predictors and adjusts R¬≤ accordingly.

###  Formula:
```
Adjusted R¬≤ = 1 - [(1 - R¬≤) * (n - 1) / (n - k - 1)]
```
- `n`: number of samples
- `k`: number of predictors

Use Adjusted R¬≤ when you have multiple features or want to prevent overfitting.

---

## üìè Evaluation Metrics

You used:
- **MSE (Mean Squared Error)**
- **MAE (Mean Absolute Error)**
- **RMSE (Root Mean Squared Error)**

These indicate how far off predictions are from the true values.

---
===================================================================================

#  Multiple Linear Regression 

This README is crafted to help you revise and understand **Multiple Linear Regression** using the California Housing dataset. It breaks down code, key metrics, and explains model behavior in simple terms.

---

##  What is Multiple Linear Regression?

It models the relationship between **one target variable (Y)** and **two or more input features (X‚ÇÅ, X‚ÇÇ, ..., X‚Çô)**:
```
Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + ... + b‚ÇôX‚Çô
```

Where:
- `Y`: Predicted output (House Price)
- `X‚ÇÅ, X‚ÇÇ, ..., X‚Çô`: Input features (e.g., MedInc, AveRooms, etc.)
- `b‚ÇÄ`: Intercept
- `b‚ÇÅ...b‚Çô`: Coefficients (weights)

---

##  Dataset Used

- **Source**: California Housing dataset from `sklearn.datasets`
- **Target**: `Price` of the house
- **Features**: MedInc, HouseAge, AveRooms, AveOccup, etc.

---

##  Steps You Performed

### 1.  Imported Libraries
```python
from sklearn.datasets import fetch_california_housing
import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt
```

### 2.  Loaded Data
```python
california = fetch_california_housing()
dataset = pd.DataFrame(california.data, columns=california.feature_names)
dataset['Price'] = california.target
```

### 3.  Exploratory Data Analysis
- Used `pairplot()` and `heatmap()` to visualize correlations.

### 4.  Splitting Features
```python
X = dataset.iloc[:, :-1]
Y = dataset.iloc[:, -1]
```

### 5.  Train-Test Split
```python
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=10)
```

### 6.  Feature Scaling
```python
from sklearn.preprocessing import StandardScaler
scalar = StandardScaler()
X_train = scalar.fit_transform(X_train)
X_test = scalar.transform(X_test)
```

### 7.  Model Training
```python
from sklearn.linear_model import LinearRegression
regression = LinearRegression()
regression.fit(X_train, Y_train)
```

### 8.  Prediction
```python
Y_pred = regression.predict(X_test)
```

---

##  Evaluation Metrics

###  Error Metrics
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error
```
- **MSE**: Average squared error
- **MAE**: Average absolute error
- **RMSE**: Root Mean Squared Error

###  R¬≤ (Coefficient of Determination)
```python
from sklearn.metrics import r2_score
r2 = r2_score(Y_test, Y_pred)
```

###  Adjusted R¬≤
```python
Adjusted = 1 - (1 - r2) * (len(Y_test) - 1) / (len(Y_test) - X_test.shape[1] - 1)
```

==============================================================================

#  Pickling 


Pickling is the process of **serializing** a Python object ‚Äî converting it into a byte stream. You can save this stream to a file or transfer it over a network, then later **unpickle** it to restore the original object.

Simply "saving your progress" so you don't have to start over.

---

##  Pickling vs Unpickling

| Operation     | Description                          | Function          |
|---------------|--------------------------------------|-------------------|
| **Pickling**   | Save Python object ‚Üí byte stream     | `pickle.dump()`   |
| **Unpickling** | Load byte stream ‚Üí Python object     | `pickle.load()`   |

---

## Your Code Example

 Trained regression model:

```python
import pickle


pickle.dump(regression, open('regressor.pkl', 'wb')) #Saving the model

model = pickle.load(open('regressor.pkl', 'rb'))#Loading the model
```

---

## Why Use Pickling?

- Saves time ‚Äî no need to retrain the model every time
-  Allows easy deployment and sharing of models
-  Works well with machine learning pipelines

--------------------------------------------------------------------------------------------------------------------
##  Logistic Regression for Binary Classification

This project focuses on solving a binary classification problem using **Logistic Regression**, a fundamental algorithm in machine learning. A synthetic dataset is generated to simulate real-world classification challenges. The model is then trained, evaluated, and optimized using hyperparameter tuning techniques.

---

###  What This Project Covers

- **Synthetic Data Creation**  
  A dataset with 1000 samples and 10 features is generated using `make_classification()` to create a controlled and customizable classification problem.

- **Exploratory Data Analysis (EDA)**  
  Visualizations and statistical summaries help understand the distribution and correlation of features.

- **Model Training with Logistic Regression**  
  Logistic Regression is applied to classify the generated data into two classes. The model learns to find the optimal decision boundary that separates them.

- **Performance Evaluation**  
  The trained model is evaluated using accuracy and a confusion matrix to understand its effectiveness in making correct predictions.

- **Hyperparameter Tuning**  
  To further improve performance, `RandomizedSearchCV` is used to search for the best combination of hyperparameters such as regularization strength and solver choice.

---

###  Expected Results

- **Accuracy Score**: A metric showing how well the model performed on unseen test data  
- **Confusion Matrix**: A table summarizing correct vs. incorrect predictions  
- **Best Hyperparameters**: Parameters found through cross-validation that enhance model performance



